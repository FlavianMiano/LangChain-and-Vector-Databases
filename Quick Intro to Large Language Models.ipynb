{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09fe376e",
   "metadata": {},
   "source": [
    "# Quick Intro to Large Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "587d389b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef4c3e",
   "metadata": {},
   "source": [
    "# Maximum number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27ca2d15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'split_text_into_chunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [5], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m max_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4097\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Split the input text into chunks based on the max tokens\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m text_chunks \u001b[38;5;241m=\u001b[39m \u001b[43msplit_text_into_chunks\u001b[49m(input_text, max_tokens)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Process each chunk separately\u001b[39;00m\n\u001b[0;32m     18\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_text_into_chunks' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Before executing the following code, make sure to have\n",
    "# your OpenAI key saved in the “OPENAI_API_KEY” environment variable.\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"Imagine a world in which humans have successfully established a colony on Mars. The Mars colony, named TerraNova, is a self-sustaining community that houses thousands of inhabitants. The colony relies on advanced technology, resource management, and scientific research to thrive in the harsh Martian environment. Describe the key infrastructure and systems that enable TerraNova to function effectively and support the needs of its residents. Discuss the transportation network, communication systems, power generation, food production, waste management, and the overall governance structure of the colony. Additionally, explore the challenges that the Mars colony faces in terms of sustainability, maintaining a habitable environment, and ensuring the well-being of its inhabitants. Provide insights into the innovations and solutions that have been implemented to overcome these challenges. Finally, speculate on the potential long-term impacts of a successful Mars colony on human civilization, space exploration, and the future of our species. Consider the scientific, social, and ethical implications of establishing a permanent settlement on another planet. Your response should be comprehensive, well-reasoned, and supported by both scientific knowledge and creative imagination.\"\n",
    "\n",
    "# Determine the maximum number of tokens from documentation\n",
    "max_tokens = 4097\n",
    "\n",
    "# Split the input text into chunks based on the max tokens\n",
    "text_chunks = split_text_into_chunks(input_text, max_tokens)\n",
    "\n",
    "# Process each chunk separately\n",
    "results = []\n",
    "for chunk in text_chunks:\n",
    "    result = llm.process(chunk)\n",
    "    results.append(result)\n",
    "\n",
    "# Combine the results as needed\n",
    "final_result = combine_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8155b1",
   "metadata": {},
   "source": [
    "# Token distribution and predicting the next token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "956060b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Happy Sockery.\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "text = 'What would be a good company name for a company that makes colorful socks?'\n",
    "\n",
    "llm = OpenAI(model_name = 'text-davinci-003', temperature = 0.7)\n",
    "\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3365192",
   "metadata": {},
   "source": [
    "# Tracking token usage ->Using callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a314e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 47\n",
      "\tPrompt Tokens: 4\n",
      "\tCompletion Tokens: 43\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.00094\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "llm = OpenAI(model_name=\"text-davinci-003\", n=2, best_of=2)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    result = llm(\"Tell me a joke\")\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28f125ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Q: What did one ocean say to the other ocean?\n",
      "A: Nothing, they just waved.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9e2bca",
   "metadata": {},
   "source": [
    "# Few-shot learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94ed4e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain import FewShotPromptTemplate\n",
    "\n",
    "# create our examples\n",
    "examples = [\n",
    "    {\n",
    "        \"query\": \"What's the weather like?\",\n",
    "        \"answer\": \"It's raining cats and dogs, better bring an umbrella!\"\n",
    "    }, {\n",
    "        \"query\": \"How old are you?\",\n",
    "        \"answer\": \"Age is just a number, but I'm timeless.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create an example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are excerpts from conversations with an AI\n",
    "assistant. The assistant is known for its humor and wit, providing\n",
    "entertaining and amusing responses to users' questions. Here are some\n",
    "examples:\n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few-shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80b79354",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's a deep question. Let me consult my philosopher algorithm... Ah, it says the meaning of life is to enjoy the journey and make the most of every moment. And also to eat lots of pizza.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "\n",
    "# load the model\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=few_shot_prompt_template)\n",
    "\n",
    "chain.run(\"What's the meaning of life?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41203e2",
   "metadata": {},
   "source": [
    "# Examples with Easy Prompts: Text Summarization, Text Translation, and Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1aca957f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "        template=template,\n",
    "    input_variables=['question']\n",
    ")\n",
    "\n",
    "# user question\n",
    "question = \"What is the capital city of France?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f82fc7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris\n"
     ]
    }
   ],
   "source": [
    "from langchain import HuggingFaceHub, LLMChain\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-large',\n",
    "    model_kwargs={'temperature':0}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about the capital of France\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee724ab",
   "metadata": {},
   "source": [
    "## Asking multiple questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec21f2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generations=[[Generation(text='paris', generation_info=None)], [Generation(text='giraffe', generation_info=None)], [Generation(text='nitrogen', generation_info=None)], [Generation(text='yellow', generation_info=None)]] llm_output=None run=RunInfo(run_id=UUID('62633da2-fd9d-41db-8c91-28daf0697f04'))\n"
     ]
    }
   ],
   "source": [
    "qa = [\n",
    "    {'question': \"What is the capital city of France?\"},\n",
    "    {'question': \"What is the largest mammal on Earth?\"},\n",
    "    {'question': \"Which gas is most abundant in Earth's atmosphere?\"},\n",
    "    {'question': \"What color is a ripe banana?\"}\n",
    "]\n",
    "res = llm_chain.generate(qa)\n",
    "print( res )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f763efd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris\\nBlue Whale\\nNitrogen\\nYellow'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_template = \"\"\"Answer the following questions one at a time.\n",
    "\n",
    "Questions:\n",
    "{questions}\n",
    "\n",
    "Answers:\n",
    "\"\"\"\n",
    "long_prompt = PromptTemplate(template=multi_template, input_variables=[\"questions\"])\n",
    "\n",
    "llm2 = OpenAI(model_name = 'text-davinci-003', temperature=0.7)\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    prompt=long_prompt,\n",
    "    llm=llm2\n",
    ")\n",
    "\n",
    "qs_str = (\n",
    "    \"What is the capital city of France?\\n\" +\n",
    "    \"What is the largest mammal on Earth?\\n\" +\n",
    "    \"Which gas is most abundant in Earth's atmosphere?\\n\" +\n",
    "        \"What color is a ripe banana?\\n\"\n",
    ")\n",
    "llm_chain.run(qs_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df59b705",
   "metadata": {},
   "source": [
    "## Text summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "474511d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "507aaa7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarization_template = \"Summarize the following text to one sentence: {text}\"\n",
    "summarization_prompt = PromptTemplate(input_variables=[\"text\"], template=summarization_template)\n",
    "summarization_chain = LLMChain(llm=llm, prompt=summarization_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0239f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"LangChain provides many modules that can be used to build language model applications. Modules can be combined to create more complex applications, or be used individually for simple applications. The most basic building block of LangChain is calling an LLM on some input. Let’s walk through a simple example of how to do this. For this purpose, let’s pretend we are building a service that generates a company name based on what the company makes.\"\n",
    "summarized_text = summarization_chain.predict(text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "65230f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain offers various modules for building language model applications, which can be used individually or combined to create complex applications, with the most basic building block being calling an LLM on input, as demonstrated in a simple example of building a company name generator.\n"
     ]
    }
   ],
   "source": [
    "print(summarized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc54f99d",
   "metadata": {},
   "source": [
    "## Text translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3128875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_template = \"Translate the following text from {source_language} to {target_language}: {text}\"\n",
    "translation_prompt = PromptTemplate(input_variables=[\"source_language\", \"target_language\", \"text\"], template=translation_template)\n",
    "translation_chain = LLMChain(llm=llm, prompt=translation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db1ad2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_language = \"English\"\n",
    "target_language = \"Kiswahili\"\n",
    "text = \"Your text here\"\n",
    "translated_text = translation_chain.predict(source_language=source_language, target_language=target_language, text=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "56749271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maandishi yako hapa\n"
     ]
    }
   ],
   "source": [
    "print(translated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e360389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
